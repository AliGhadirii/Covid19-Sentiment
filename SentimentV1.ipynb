{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentV1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AliGhadirii/Covid19-Sentiment/blob/main/SentimentV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BDijP1_xv7X",
        "outputId": "e4615d93-6717-40ab-f2ff-85d7fcf8bcda"
      },
      "source": [
        "!pip install transformers==3.0.0\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.0.0 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.1.95)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.8.0rc4)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMZY2CAngHu7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "320be19a-f39a-45bb-a3ed-79d77c3edc42"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "import transformers\r\n",
        "from transformers import AutoModel, BertTokenizerFast\r\n",
        "from transformers import AdamW\r\n",
        "from sklearn.utils.class_weight import compute_class_weight\r\n",
        "from pytorch_transformers import BertConfig, BertForSequenceClassification\r\n",
        "\r\n",
        "# specify GPU\r\n",
        "device = torch.device(\"cuda\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-84-666a604dfda1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# specify GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB8lgSxgxDf_"
      },
      "source": [
        "df = pd.read_csv(\"SST5_lower.csv\")\r\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \r\n",
        "                                                                    random_state=2021, \r\n",
        "                                                                    test_size=0.3, \r\n",
        "                                                                    stratify=df['label'])\r\n",
        "\r\n",
        "# we will use temp_text and temp_labels to create validation and test set, 50% each\r\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \r\n",
        "                                                                    random_state=2021, \r\n",
        "                                                                    test_size=0.5, \r\n",
        "                                                                    stratify=temp_labels)\r\n",
        "\r\n",
        "seq_len = [len(i.split()) for i in train_text]\r\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6n5SV5px-5k"
      },
      "source": [
        "max_seq_len = 25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNwbDXgAzD4L"
      },
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased')\r\n",
        "\r\n",
        "tokens_train = tokenizer.batch_encode_plus(\r\n",
        "    train_text.tolist(),\r\n",
        "    max_length = max_seq_len,\r\n",
        "    pad_to_max_length=True,\r\n",
        "    truncation=True,\r\n",
        "    return_token_type_ids=False\r\n",
        ")\r\n",
        "\r\n",
        "# tokenize and encode sequences in the validation set\r\n",
        "tokens_val = tokenizer.batch_encode_plus(\r\n",
        "    val_text.tolist(),\r\n",
        "    max_length = max_seq_len,\r\n",
        "    pad_to_max_length=True,\r\n",
        "    truncation=True,\r\n",
        "    return_token_type_ids=False\r\n",
        ")\r\n",
        "\r\n",
        "# tokenize and encode sequences in the test set\r\n",
        "tokens_test = tokenizer.batch_encode_plus(\r\n",
        "    test_text.tolist(),\r\n",
        "    max_length = max_seq_len,\r\n",
        "    pad_to_max_length=True,\r\n",
        "    truncation=True,\r\n",
        "    return_token_type_ids=False\r\n",
        ")\r\n",
        "\r\n",
        "\r\n",
        "# for train set\r\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\r\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\r\n",
        "train_y = torch.tensor(train_labels.tolist())\r\n",
        "\r\n",
        "# for validation set\r\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\r\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\r\n",
        "val_y = torch.tensor(val_labels.tolist())\r\n",
        "\r\n",
        "# for test set\r\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\r\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\r\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZFhA-RezHah"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "#define a batch size\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# wrap tensors\r\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\r\n",
        "\r\n",
        "# sampler for sampling the data during training\r\n",
        "train_sampler = RandomSampler(train_data)\r\n",
        "\r\n",
        "# dataLoader for train set\r\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n",
        "\r\n",
        "# wrap tensors\r\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\r\n",
        "\r\n",
        "# sampler for sampling the data during training\r\n",
        "val_sampler = SequentialSampler(val_data)\r\n",
        "\r\n",
        "# dataLoader for validation set\r\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMv1iC2oz2Af"
      },
      "source": [
        "# bert = AutoModel.from_pretrained('bert-large-uncased')\r\n",
        "\r\n",
        "# for param in bert.parameters():\r\n",
        "#     param.requires_grad = False\r\n",
        "\r\n",
        "config = BertConfig.from_pretrained('bert-large-uncased')\r\n",
        "model = BertForSequenceClassification.from_pretrained('bert-large-uncased', config=config)\r\n",
        "\r\n",
        "model = model.to(device)\r\n",
        "lossfn = torch.nn.CrossEntropyLoss()\r\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YII466Rz4RM"
      },
      "source": [
        "class BERT_Arch(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, bert):\r\n",
        "      \r\n",
        "      super(BERT_Arch, self).__init__()\r\n",
        "    \r\n",
        "      self.bert = bert \r\n",
        "      \r\n",
        "      # dropout layer\r\n",
        "      self.dropout = nn.Dropout(0.1)\r\n",
        "      \r\n",
        "      # relu activation function\r\n",
        "      self.relu =  nn.ReLU()\r\n",
        "\r\n",
        "      # dense layer 1\r\n",
        "      self.fc1 = nn.Linear(1024,512)\r\n",
        "      # dense layer 2\r\n",
        "      self.fc2 = nn.Linear(512,256)\r\n",
        "      # dense layer 3\r\n",
        "      self.fc3 = nn.Linear(256,128)\r\n",
        "      # dense layer 4\r\n",
        "      self.fc4 = nn.Linear(128,64)\r\n",
        "      # dense layer 5 (Output layer)\r\n",
        "      self.fc5 = nn.Linear(64,5)\r\n",
        "\r\n",
        "    #define the forward pass\r\n",
        "    def forward(self, sent_id, mask):\r\n",
        "\r\n",
        "      #pass the inputs to the model  \r\n",
        "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\r\n",
        "      \r\n",
        "      x = self.fc1(cls_hs)\r\n",
        "\r\n",
        "      x = self.relu(x)\r\n",
        "\r\n",
        "      x = self.dropout(x)\r\n",
        "\r\n",
        "      x = self.fc2(x)\r\n",
        "\r\n",
        "      x = self.relu(x)\r\n",
        "\r\n",
        "      x = self.dropout(x)\r\n",
        "      x = self.fc3(x)\r\n",
        "\r\n",
        "      x = self.relu(x)\r\n",
        "\r\n",
        "      x = self.dropout(x)\r\n",
        "      x = self.fc4(x)\r\n",
        "\r\n",
        "      x = self.relu(x)\r\n",
        "\r\n",
        "      x = self.dropout(x)\r\n",
        "      # output layer\r\n",
        "      x = self.fc5(x)\r\n",
        "\r\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bJs9UGZ0G4Q"
      },
      "source": [
        "#model = BERT_Arch(bert)\r\n",
        "# push the model to GPU\r\n",
        "\r\n",
        "#model = model.to(device)\r\n",
        "\r\n",
        "#optimizer = AdamW(model.parameters(), lr = 1e-3)\r\n",
        "#class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\r\n",
        "\r\n",
        "# convert class weights to tensor\r\n",
        "#weights= torch.tensor(class_wts,dtype=torch.float)\r\n",
        "#weights = weights.to(device)\r\n",
        "\r\n",
        "# loss function\r\n",
        "#cross_entropy  = nn.CrossEntropyLoss(weight=weights).to(device)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQe9tFYY0JrC"
      },
      "source": [
        "# function to train the model\r\n",
        "def train():\r\n",
        "  \r\n",
        "  model.train()\r\n",
        "\r\n",
        "  total_loss, total_accuracy = 0, 0\r\n",
        "  \r\n",
        "  # empty list to save model predictions\r\n",
        "  total_preds=[]\r\n",
        "  \r\n",
        "  # iterate over batches\r\n",
        "  for step,batch in enumerate(train_dataloader):\r\n",
        "    \r\n",
        "    # progress update after every 50 batches.\r\n",
        "    if step % 50 == 0 and not step == 0:\r\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\r\n",
        "\r\n",
        "    # push the batch to gpu\r\n",
        "    batch = [r.to(device) for r in batch]\r\n",
        " \r\n",
        "    sent_id, mask, labels = batch\r\n",
        "\r\n",
        "    # clear previously calculated gradients \r\n",
        "    model.zero_grad()        \r\n",
        "\r\n",
        "    # get model predictions for the current batch\r\n",
        "    preds = model(sent_id, mask)\r\n",
        "\r\n",
        "    # compute the loss between actual and predicted values\r\n",
        "    loss = lossfn(preds, labels)\r\n",
        "\r\n",
        "    # add on to the total loss\r\n",
        "    total_loss = total_loss + loss.item()\r\n",
        "\r\n",
        "    # backward pass to calculate the gradients\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\r\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "    # update parameters\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # model predictions are stored on GPU. So, push it to CPU\r\n",
        "    preds=preds.detach().cpu().numpy()\r\n",
        "\r\n",
        "    # append the model predictions\r\n",
        "    total_preds.append(preds)\r\n",
        "\r\n",
        "  # compute the training loss of the epoch\r\n",
        "  avg_loss = total_loss / len(train_dataloader)\r\n",
        "  \r\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\r\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\r\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\r\n",
        "\r\n",
        "  #returns the loss and predictions\r\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxMZAzwb0vfP"
      },
      "source": [
        "def evaluate():\r\n",
        "  \r\n",
        "  print(\"\\nEvaluating...\")\r\n",
        "  \r\n",
        "  # deactivate dropout layers\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  total_loss, total_accuracy = 0, 0\r\n",
        "  \r\n",
        "  # empty list to save the model predictions\r\n",
        "  total_preds = []\r\n",
        "\r\n",
        "  # iterate over batches\r\n",
        "  for step,batch in enumerate(val_dataloader):\r\n",
        "    \r\n",
        "    # Progress update every 50 batches.\r\n",
        "    if step % 50 == 0 and not step == 0:\r\n",
        "      # Report progress.\r\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\r\n",
        "\r\n",
        "    # push the batch to gpu\r\n",
        "    batch = [t.to(device) for t in batch]\r\n",
        "\r\n",
        "    sent_id, mask, labels = batch\r\n",
        "\r\n",
        "    # deactivate autograd\r\n",
        "    with torch.no_grad():\r\n",
        "      \r\n",
        "      # model predictions\r\n",
        "      preds = model(sent_id, mask)\r\n",
        "\r\n",
        "      # compute the validation loss between actual and predicted values\r\n",
        "      loss = lossfn(preds,labels)\r\n",
        "\r\n",
        "      total_loss = total_loss + loss.item()\r\n",
        "\r\n",
        "      preds = preds.detach().cpu().numpy()\r\n",
        "\r\n",
        "      total_preds.append(preds)\r\n",
        "\r\n",
        "  # compute the validation loss of the epoch\r\n",
        "  avg_loss = total_loss / len(val_dataloader) \r\n",
        "\r\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\r\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\r\n",
        "\r\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gm7VHgIQ1eq6",
        "outputId": "6709b5f8-5852-4ae4-c61e-20cc93648997"
      },
      "source": [
        "# number of training epochs\r\n",
        "epochs = 10\r\n",
        "# set initial loss to infinite\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "# empty lists to store training and validation loss of each epoch\r\n",
        "train_losses=[]\r\n",
        "valid_losses=[]\r\n",
        "\r\n",
        "#for each epoch\r\n",
        "for epoch in range(epochs):\r\n",
        "     \r\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\r\n",
        "    \r\n",
        "    #train model\r\n",
        "    train_loss, _ = train()\r\n",
        "    \r\n",
        "    #evaluate model\r\n",
        "    valid_loss, _ = evaluate()\r\n",
        "    \r\n",
        "    #save the best model\r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\r\n",
        "    \r\n",
        "    # append training and validation loss\r\n",
        "    train_losses.append(train_loss)\r\n",
        "    valid_losses.append(valid_loss)\r\n",
        "    \r\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f}')\r\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch    50  of    260.\n",
            "  Batch   100  of    260.\n",
            "  Batch   150  of    260.\n",
            "  Batch   200  of    260.\n",
            "  Batch   250  of    260.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     56.\n",
            "\n",
            "Training Loss: 1.442\n",
            "Validation Loss: 1.407\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch    50  of    260.\n",
            "  Batch   100  of    260.\n",
            "  Batch   150  of    260.\n",
            "  Batch   200  of    260.\n",
            "  Batch   250  of    260.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     56.\n",
            "\n",
            "Training Loss: 1.415\n",
            "Validation Loss: 1.404\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch    50  of    260.\n",
            "  Batch   100  of    260.\n",
            "  Batch   150  of    260.\n",
            "  Batch   200  of    260.\n",
            "  Batch   250  of    260.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     56.\n",
            "\n",
            "Training Loss: 1.416\n",
            "Validation Loss: 1.498\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch    50  of    260.\n",
            "  Batch   100  of    260.\n",
            "  Batch   150  of    260.\n",
            "  Batch   200  of    260.\n",
            "  Batch   250  of    260.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     56.\n",
            "\n",
            "Training Loss: 1.445\n",
            "Validation Loss: 1.679\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch    50  of    260.\n",
            "  Batch   100  of    260.\n",
            "  Batch   150  of    260.\n",
            "  Batch   200  of    260.\n",
            "  Batch   250  of    260.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     56.\n",
            "\n",
            "Training Loss: 1.462\n",
            "Validation Loss: 1.432\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch    50  of    260.\n",
            "  Batch   100  of    260.\n",
            "  Batch   150  of    260.\n",
            "  Batch   200  of    260.\n",
            "  Batch   250  of    260.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     56.\n",
            "\n",
            "Training Loss: 1.441\n",
            "Validation Loss: 1.392\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch    50  of    260.\n",
            "  Batch   100  of    260.\n",
            "  Batch   150  of    260.\n",
            "  Batch   200  of    260.\n",
            "  Batch   250  of    260.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     56.\n",
            "\n",
            "Training Loss: 1.438\n",
            "Validation Loss: 1.400\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch    50  of    260.\n",
            "  Batch   100  of    260.\n",
            "  Batch   150  of    260.\n",
            "  Batch   200  of    260.\n",
            "  Batch   250  of    260.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     56.\n",
            "\n",
            "Training Loss: 1.417\n",
            "Validation Loss: 1.383\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch    50  of    260.\n",
            "  Batch   100  of    260.\n",
            "  Batch   150  of    260.\n",
            "  Batch   200  of    260.\n",
            "  Batch   250  of    260.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     56.\n",
            "\n",
            "Training Loss: 1.417\n",
            "Validation Loss: 1.371\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch    50  of    260.\n",
            "  Batch   100  of    260.\n",
            "  Batch   150  of    260.\n",
            "  Batch   200  of    260.\n",
            "  Batch   250  of    260.\n",
            "\n",
            "Evaluating...\n",
            "  Batch    50  of     56.\n",
            "\n",
            "Training Loss: 1.440\n",
            "Validation Loss: 1.427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0WsBulH44EA"
      },
      "source": [
        "path = 'saved_weights.pt'\r\n",
        "model.load_state_dict(torch.load(path))\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\r\n",
        "  preds = preds.detach().cpu().numpy()\r\n",
        "  preds = np.argmax(preds, axis = 1)\r\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}